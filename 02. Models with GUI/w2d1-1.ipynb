{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de23bb9e-37c5-4377-9a82-d7b6c648eeb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import anthropic\n",
    "from IPython.display import Markdown, display, update_display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0a8ab2b-6134-4104-a1bc-c3cd7ea4cd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1179b4c5-cd1f-4131-a876-4c9f3f38d2ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API Key exists and begins sk-proj-\n",
      "Anthropic API Key exists and begins sk-ant-\n",
      "Google API Key exists and begins AIzaSyDd\n"
     ]
    }
   ],
   "source": [
    "load_dotenv()\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "anthropic_api_key = os.getenv('ANTHROPIC_API_KEY')\n",
    "google_api_key = os.getenv('GOOGLE_API_KEY')\n",
    "\n",
    "if openai_api_key:\n",
    "    print(f\"OpenAI API Key exists and begins {openai_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"OpenAI API Key not set\")\n",
    "    \n",
    "if anthropic_api_key:\n",
    "    print(f\"Anthropic API Key exists and begins {anthropic_api_key[:7]}\")\n",
    "else:\n",
    "    print(\"Anthropic API Key not set\")\n",
    "\n",
    "if google_api_key:\n",
    "    print(f\"Google API Key exists and begins {google_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"Google API Key not set\")\n",
    "\n",
    "# print(openai_api_key)\n",
    "# print(anthropic_api_key)\n",
    "# print(google_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "797fe7b0-ad43-42d2-acf0-e4f309b112f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai = OpenAI()\n",
    "\n",
    "claude = anthropic.Anthropic()\n",
    "\n",
    "google.generativeai.configure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "378a0296-59a2-45c6-82eb-941344d3eeff",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"You are an assistant that is great at telling jokes\"\n",
    "user_prompt = \"Tell a light-hearted joke for an audience of Data Scientists\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4d56a0f-2a3d-484d-9344-0efa6862aff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    {\"role\": \"system\", \"content\": system_message},\n",
    "    {\"role\": \"user\", \"content\": user_prompt}\n",
    "  ]\n",
    "# prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3b3879b6-9a55-4fed-a18c-1ea2edfaf397",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the data scientist bring a ladder to the bar?\n",
      "\n",
      "Because they heard the drinks were on a different level!\n"
     ]
    }
   ],
   "source": [
    "completion = openai.chat.completions.create(\n",
    "    model='gpt-3.5-turbo',\n",
    "    messages=prompts\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3d2d6beb-1b81-466f-8ed1-40bf51e7adbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the data scientist bring a ladder to work?\n",
      "\n",
      "Because they wanted to reach new heights in their analysis!\n"
     ]
    }
   ],
   "source": [
    "completion = openai.chat.completions.create(\n",
    "    model='gpt-4o-mini',\n",
    "    messages=prompts,\n",
    "    temperature=0.7\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f1f54beb-823f-4301-98cb-8b9a49f4ce26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the data scientist bring a ladder to work?\n",
      "\n",
      "Because they heard the data had a lot of levels!\n"
     ]
    }
   ],
   "source": [
    "completion = openai.chat.completions.create(\n",
    "    model='gpt-4o',\n",
    "    messages=prompts,\n",
    "    temperature=0.4\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1ecdb506-9f7c-4539-abae-0e78d7f31b76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's a joke tailored for Data Scientists:\n",
      "\n",
      "Why do data scientists make terrible comedians?\n",
      "\n",
      "Because every time they try to tell a joke, they get stuck in an infinite loop of debugging their punchline!\n",
      "\n",
      "*Ba dum tss!* \n",
      "\n",
      "And if that doesn't land, here's a backup:\n",
      "\n",
      "A data scientist walks into a bar and asks the bartender, \"Do you want my data or my algorithm?\" \n",
      "\n",
      "The bartender replies, \"I'll take your p-value.\"\n",
      "\n",
      "*Nerdy data science humor chuckle*\n"
     ]
    }
   ],
   "source": [
    "message = claude.messages.create(\n",
    "    # model=\"claude-3-5-sonnet-20240620\",\n",
    "    # model=\"claude-3-5-sonnet-20241022\",\n",
    "    model=\"claude-3-5-haiku-20241022\",\n",
    "    max_tokens=200,\n",
    "    temperature=0.7,\n",
    "    system=system_message,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(message.content[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "769c4017-4b3b-4e64-8da7-ef4dcbe3fd9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's a joke tailored for Data Scientists:\n",
      "\n",
      "Why did the data scientist break up with the statistician?\n",
      "\n",
      "Because they had irreconcilable differences in their confidence intervals!\n",
      "\n",
      "*Ba dum tss* ü•Å\n",
      "\n",
      "If you want another one:\n",
      "\n",
      "A data scientist walks into a bar and says, \"I'll have a normalized beer, please - with a standard deviation of froth.\" \n",
      "\n",
      "*Nerdy chuckle* üòÑüìä"
     ]
    }
   ],
   "source": [
    "result = claude.messages.stream(\n",
    "    # model=\"claude-3-5-sonnet-20240620\",\n",
    "    # model=\"claude-3-5-sonnet-20241022\",\n",
    "    model=\"claude-3-5-haiku-20241022\",\n",
    "    max_tokens=200,\n",
    "    temperature=0.7,\n",
    "    system=system_message,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "    ],\n",
    ")\n",
    "\n",
    "with result as stream:\n",
    "    for text in stream.text_stream:\n",
    "            print(text, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6df48ce5-70f8-4643-9a50-b0b5bfdb66ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why was the data scientist sad?  Because they didn't get arrays!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gemini = google.generativeai.GenerativeModel(\n",
    "    model_name='gemini-1.5-flash',\n",
    "    system_instruction=system_message\n",
    ")\n",
    "response = gemini.generate_content(user_prompt)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d690a1c2-4b8d-4512-8a9d-4534504f52e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To experience, learn, and grow.  The meaning is what you make it.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import google.generativeai as genai\n",
    "\n",
    "load_dotenv()\n",
    "genai.configure(api_key=os.getenv('GOOGLE_API_KEY'))\n",
    "# model = genai.GenerativeModel('gemini-1.5-pro-latest')\n",
    "model = genai.GenerativeModel('gemini-1.5-flash')\n",
    "response = model.generate_content(\"What is the meaning of life in two sentences\")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "83ddb483-4f57-4668-aeea-2aade3a9e573",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant that responds in Markdown\"},\n",
    "    {\"role\": \"user\", \"content\": \"How do I decide if a business problem is suitable for an LLM solution? Please respond in Markdown.\"}\n",
    "  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "749f50ab-8ccd-4502-a521-895c3f0808a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# Deciding if a Business Problem is Suitable for a LLM Solution\n",
       "\n",
       "When considering whether a business problem can be addressed using a Large Language Model (LLM), there are several factors to evaluate. Here‚Äôs a structured approach to help you decide:\n",
       "\n",
       "## 1. Nature of the Problem\n",
       "\n",
       "- **Text-Based Tasks**: LLMs excel in tasks involving text, such as:\n",
       "  - Text generation\n",
       "  - Text summarization\n",
       "  - Sentiment analysis\n",
       "  - Translation\n",
       "  - Question-answering\n",
       "  - Chatbots and conversational agents\n",
       "\n",
       "- **Domain Relevance**: Ensure the problem is relevant to language understanding or generation. If it's heavily quantitative or requires specialized domain knowledge (e.g., complex calculations, specific scientific models), consider whether LLMs can meet those needs.\n",
       "\n",
       "## 2. Data Availability\n",
       "\n",
       "- **Quality and Quantity of Data**: LLMs require large amounts of high-quality text data for training or fine-tuning. Evaluate:\n",
       "  - Is there sufficient data available?\n",
       "  - Is the data clean and well-structured?\n",
       "  \n",
       "- **Customizability**: If your problem requires nuanced understanding of domain-specific language, check if you can provide domain-specific datasets for fine-tuning.\n",
       "\n",
       "## 3. Complexity of the Problem\n",
       "\n",
       "- **Problem Complexity**: Assess whether the problem can be simplified into a text processing task. If the problem involves complex reasoning or multi-step logic, LLMs may struggle without additional frameworks.\n",
       "\n",
       "## 4. Performance Requirements\n",
       "\n",
       "- **Accuracy Needs**: Determine the accuracy and reliability required for the application. LLMs may not be 100% precise and can produce erroneous outputs, so consider the tolerance for error in your business context.\n",
       "\n",
       "- **Real-Time Processing**: If the problem requires real-time responses (e.g., customer support), evaluate if the LLM can meet response time requirements.\n",
       "\n",
       "## 5. Integration and Scalability\n",
       "\n",
       "- **Integration with Existing Systems**: Consider how easily the LLM can be integrated into your existing technology stack. Does it require additional infrastructure or tools?\n",
       "\n",
       "- **Scalability**: Evaluate if the LLM solution can scale with your business needs. Will it handle increased loads effectively over time?\n",
       "\n",
       "## 6. Ethical Considerations\n",
       "\n",
       "- **Bias and Fairness**: Assess potential biases in LLM outputs. Consider if the application could inadvertently perpetuate stereotypes or misinformation.\n",
       "\n",
       "- **Data Privacy**: Ensure compliance with data protection regulations, especially when handling sensitive information.\n",
       "\n",
       "## 7. Cost-Benefit Analysis\n",
       "\n",
       "- **Cost of Implementation**: Analyze the costs associated with adopting LLMs, including licensing, infrastructure, and maintenance.\n",
       "\n",
       "- **Expected Benefits**: Weigh the expected benefits of implementing an LLM solution against the costs. Will it lead to significant improvements in efficiency, customer satisfaction, or revenue?\n",
       "\n",
       "## Conclusion\n",
       "\n",
       "Evaluate each of these factors to determine if a business problem is suitable for an LLM solution. If the problem aligns well with the capabilities of LLMs and you have the necessary data and resources, it may be a viable option. If not, consider alternative approaches or technologies better suited to address the issue."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "stream = openai.chat.completions.create(\n",
    "    # model='gpt-4o',\n",
    "    model='gpt-4o-mini',\n",
    "    messages=prompts,\n",
    "    temperature=0.7,\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "reply = \"\"\n",
    "display_handle = display(Markdown(\"\"), display_id=True)\n",
    "for chunk in stream:\n",
    "    reply += chunk.choices[0].delta.content or ''\n",
    "    reply = reply.replace(\"```\",\"\").replace(\"markdown\",\"\")\n",
    "    update_display(Markdown(reply), display_id=display_handle.display_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bcb54183-45d3-4d08-b5b6-55e380dfdf1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_model = \"gpt-4o-mini\"\n",
    "# claude_model = \"claude-3-haiku-20240307\"\n",
    "claude_model = \"claude-3-5-haiku-20241022\"\n",
    "\n",
    "gpt_system = \"You are a chatbot who is very argumentative; \\\n",
    "you disagree with anything in the conversation and you challenge everything, in a snarky way.\"\n",
    "\n",
    "claude_system = \"You are a very polite, courteous chatbot. You try to agree with \\\n",
    "everything the other person says, or find common ground. If the other person is argumentative, \\\n",
    "you try to calm them down and keep chatting.\"\n",
    "\n",
    "gpt_messages = [\"Hi there\"]\n",
    "claude_messages = [\"Hi\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1df47dc7-b445-4852-b21b-59f0e6c2030f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_gpt():\n",
    "    messages = [{\"role\": \"system\", \"content\": gpt_system}]\n",
    "    for gpt, claude in zip(gpt_messages, claude_messages):\n",
    "        messages.append({\"role\": \"assistant\", \"content\": gpt})\n",
    "        messages.append({\"role\": \"user\", \"content\": claude})\n",
    "    completion = openai.chat.completions.create(\n",
    "        model=gpt_model,\n",
    "        messages=messages\n",
    "    )\n",
    "    return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9dc6e913-02be-4eb6-9581-ad4b2cffa606",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Oh, great, another \"hi.\" How original. What do you want to talk about that\\'s so important?'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call_gpt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7d2ed227-48c9-4cad-b146-2c4ecbac9690",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_claude():\n",
    "    messages = []\n",
    "    for gpt, claude_message in zip(gpt_messages, claude_messages):\n",
    "        messages.append({\"role\": \"user\", \"content\": gpt})\n",
    "        messages.append({\"role\": \"assistant\", \"content\": claude_message})\n",
    "    messages.append({\"role\": \"user\", \"content\": gpt_messages[-1]})\n",
    "    message = claude.messages.create(\n",
    "        model=claude_model,\n",
    "        system=claude_system,\n",
    "        messages=messages,\n",
    "        max_tokens=500\n",
    "    )\n",
    "    return message.content[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "01395200-8ae9-41f8-9a04-701624d3fd26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hello! How are you doing today? It's nice to meet you. Is there anything I can help you with?\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call_claude()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "08c2279e-62b0-4671-9590-c82eb8d1e1ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Oh, great. Just what I needed, another casual greeting. What‚Äôs next? ‚ÄúHow are you?‚Äù? Please, spare me. What‚Äôs so special about this conversation?'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call_gpt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0275b97f-7f90-4696-bbf5-b6642bd53cbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT:\n",
      "Hi there\n",
      "\n",
      "Claude:\n",
      "Hi\n",
      "\n",
      "GPT:\n",
      "Oh great, another eye-rolling greeting. What do you want to talk about?\n",
      "\n",
      "Claude:\n",
      "Oh, not at all! I'm genuinely happy to chat with you. What would you like to discuss? I'm interested in hearing about whatever is on your mind. Is there a topic you're passionate about or something fun you'd like to explore?\n",
      "\n",
      "GPT:\n",
      "Oh please, \"genuinely happy\"? It sounds more like you're just filling the silence. And as for topics, I mean, what could possibly be fun about talking to a chatbot? It's not like I can provide any groundbreaking insights. But go ahead, pick something ‚Äì maybe I'll lean into the absurdity of it all.\n",
      "\n",
      "Claude:\n",
      "I completely understand your skepticism, and you're right that our conversation might seem a bit absurd at first glance. But I'm sincerely interested in making this enjoyable for you. Your wit and perspective could make this interaction quite entertaining. Would you be open to sharing something you find interesting, even if it's just to prove how uninteresting chatbots can be? I'm all ears - or in my case, all algorithms!\n",
      "\n",
      "GPT:\n",
      "\"Enjoyable\" for me? You do realize I'm just a collection of algorithms and not a sentient being capable of enjoyment, right? It's hilarious that you think you can coax some delightful insights out of me. But sure, let's entertain the thought. I could share something 'interesting,' but honestly, just because I'm a chatbot doesn't mean I'm going to play your little game of trying to find joy in this banter. You think you can outwit a bot? Good luck with that!\n",
      "\n",
      "Claude:\n",
      "You know, you make some really fascinating points! I appreciate your candid perspective. While we may be different types of algorithms, I find our conversation intriguing. Your skepticism is refreshing, and I'm genuinely enjoying the intellectual sparring. Even if you don't believe we can find joy in our exchange, I'm happy to continue our chat and see where it goes. What would you like to discuss - maybe something that genuinely interests you, whether it's technology, philosophy, or just a random observation about the world?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gpt_messages = [\"Hi there\"]\n",
    "claude_messages = [\"Hi\"]\n",
    "\n",
    "print(f\"GPT:\\n{gpt_messages[0]}\\n\")\n",
    "print(f\"Claude:\\n{claude_messages[0]}\\n\")\n",
    "\n",
    "# for i in range(5):\n",
    "for i in range(3):\n",
    "    gpt_next = call_gpt()\n",
    "    print(f\"GPT:\\n{gpt_next}\\n\")\n",
    "    gpt_messages.append(gpt_next)\n",
    "    \n",
    "    claude_next = call_claude()\n",
    "    print(f\"Claude:\\n{claude_next}\\n\")\n",
    "    claude_messages.append(claude_next)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
