{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d733e1a-fff5-4739-b702-cc4af0a69d41",
   "metadata": {},
   "source": [
    "# Main Operations with LCEL Chains"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2bc3c9c-e443-4a53-a3d6-98d8d23f76f3",
   "metadata": {},
   "source": [
    "## Contents\n",
    "* Chaining runnables, coercion\n",
    "* Multiple chains\n",
    "* Nested chains\n",
    "* Fallback for chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "992ec4a9-aa01-4e44-aeb9-b9a1f3aa9e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n",
    "openai_api_key = os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c03febb1-c3de-485a-ae2d-b68066790912",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "langchain                 0.2.3\n",
      "langchain-community       0.2.4\n",
      "langchain-core            0.2.5\n",
      "langchain-openai          0.1.8\n",
      "langchain-text-splitters  0.2.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.11 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip list | grep langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc2af3ef-c2c7-445f-92bd-a29c68abce25",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Connect with LLM and start conversation with it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9afcbc7d-a816-41e3-925f-850883f5770d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-3.5-turbo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "349b84fd-b4c6-442f-bade-e801e6c7ad81",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Basic chain: LLM model + prompt + output parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7aa73509-0251-4951-9f9b-54ac1bcd73fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"Write one brief sentence about {politician}\")\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "chain = prompt | model | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "08f3c0a8-bc37-45da-bd60-871f6d60c7cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'John F. Kennedy was the 35th President of the United States, serving from 1961 until his assassination in 1963.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\n",
    "    \"politician\": \"JFK\"\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654f35c4-015d-4a0a-8d55-ad70046c9f40",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Mid-level chain: Retriever App Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2aadaa7e-42e3-45c7-bacf-c3047a1fefe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install docarray tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6bdb403a-7cc7-4876-aa2b-07356bba0e35",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tiktoken                  0.7.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.11 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip list | grep tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "73a3e0d5-a939-4339-a5dd-3a050087859b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/juliocolomer/.pyenv/versions/3.11.4/envs/venv061224-p1/lib/python3.11/site-packages/pydantic/_migration.py:283: UserWarning: `pydantic.error_wrappers:ValidationError` has been moved to `pydantic:ValidationError`.\n",
      "  warnings.warn(f'`{import_path}` has been moved to `{new_location}`.')\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.vectorstores import DocArrayInMemorySearch\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "vectorstore = DocArrayInMemorySearch.from_texts(\n",
    "    [\"AI Accelera has provided Generative AI Training and Consulting Services in more than 100 countries\", \"Aceleradora AI is the branch of AI Accelera for the Spanish-Speaking market\"],\n",
    "    embedding=OpenAIEmbeddings(),\n",
    ")\n",
    "\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "output_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "739774c6-5ec1-4f07-96a1-ec478d99ecee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "\n",
    "get_question_and_retrieve_relevant_docs = RunnableParallel(\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763e71c5-6625-4f3e-ac66-ed0648e6f3a6",
   "metadata": {},
   "source": [
    "* The previous code creates a `RunnableParallel` object with two entries.\n",
    "    * The first entry, context, will include the document results fetched by the retriever.\n",
    "    * The second entry, question, will contain the user’s original question. To pass on the question, we use `RunnablePassthrough` to copy this entry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ec29ebf1-9b4b-471e-b004-41ecfa0f4d29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'AI Accelera has provided services in more than 100 countries.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = get_question_and_retrieve_relevant_docs | prompt | model | output_parser\n",
    "\n",
    "chain.invoke(\"In how many countries has AI Accelera provided services?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a43fc04-90ca-4dcf-9aaf-3e1856b94f2c",
   "metadata": {},
   "source": [
    "* Graphic representation of what this chain does:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c07e27-69e8-4e02-ad9f-9d3b738c1693",
   "metadata": {},
   "source": [
    "![alt text](LCEL-chain-graph.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830adc08-3c75-4e46-9754-355e15ea2fdf",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## RunnableParallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dbbe5383-9ac3-4c3c-b722-7a536cf2425a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "199d6899-a0bb-4a8f-b58d-a86eba2e3455",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Alumni of AI Accelera are individuals from all continents and top companies, totaling more than 7,000 trained.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableParallel\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "\n",
    "vectorstore = FAISS.from_texts(\n",
    "    [\"AI Accelera has trained more than 7.000 Alumni from all continents and top companies\"], embedding=OpenAIEmbeddings()\n",
    ")\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "\n",
    "retrieval_chain = (\n",
    "    RunnableParallel({\"context\": retriever, \"question\": RunnablePassthrough()})\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "retrieval_chain.invoke(\"who are the Alumni of AI Accelera?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f675d1d4-5d30-45a0-8a2a-7e163627109b",
   "metadata": {},
   "source": [
    "### Rememeber: Syntax of RunnableParallel can have several variations\n",
    "* When composing RunnableParallel with another Runnable we do not need to wrap it up in RunnableParallel class. Inside chain, next three syntaxs are equivalent:\n",
    "    * `RunnableParallel({\"context\": retriever, \"question\": RunnablePassthrough()})`\n",
    "    * `RunnableParallel(context=retriever, question=RunnablePassthrough())`\n",
    "    * `{\"context\": retriever, \"question\": RunnablePassthrough()}`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7afcf618-a9bc-40ce-b3ab-f970ee1bbadc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Using itemgetter with RunnableParallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b16e9cca-a8a3-4f37-bd2e-fc8a9c0c1adc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Arrr, AI Accelera has trained more than 3,000 Enterprise Alumni.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "\n",
    "vectorstore = FAISS.from_texts(\n",
    "    [\"AI Accelera has trained more than 3,000 Enterprise Alumni.\"], embedding=OpenAIEmbeddings()\n",
    ")\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer in the following language: {language}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "chain = (\n",
    "    {\n",
    "        \"context\": itemgetter(\"question\") | retriever,\n",
    "        \"question\": itemgetter(\"question\"),\n",
    "        \"language\": itemgetter(\"language\"),\n",
    "    }\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "chain.invoke({\"question\": \"How many Enterprise Alumni has trained AI Accelera?\", \"language\": \"Pirate English\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c333cb-4d5e-41b3-b007-8c4b1d175a93",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## RunnablePassthrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f13d185f-e00d-4842-9c99-87f3d4da7a10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'user_input': {'num': 1}, 'transformed_output': 2}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "\n",
    "runnable = RunnableParallel(\n",
    "    user_input = RunnablePassthrough(),\n",
    "    transformed_output = lambda x: x[\"num\"] + 1,\n",
    ")\n",
    "\n",
    "runnable.invoke({\"num\": 1})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa7f207a-fdb9-46cf-b1c0-06fbcf9b5e99",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Chaining Runnables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "51ba28a5-a605-482a-b3df-43a70e4dc352",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"tell me a sentence about {politician}\")\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "\n",
    "chain = prompt | model | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1d91592b-027a-4ea4-a764-94bf27b8610f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Chamberlain was a British politician who is best known for his policy of appeasement towards Nazi Germany in the years leading up to World War II.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\"Chamberlain\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82645057-4570-4dd9-ad3b-58f0365bb4eb",
   "metadata": {},
   "source": [
    "### Coercion: Combine chain (which is Runnable) with other Runnables to create new chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9c8f1937-227a-4d4f-9e62-2b6792f00541",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "historian_prompt = ChatPromptTemplate.from_template(\"Was {politician} positive for Humanity?\")\n",
    "\n",
    "composed_chain = {\"politician\": chain} | historian_prompt | model | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f98a3705-a6b0-473f-b4b8-7edd2dff9831",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Yes, Abraham Lincoln is considered to have had a positive impact on humanity. His leadership during the Civil War helped to preserve the unity of the United States and ultimately led to the end of slavery. The Emancipation Proclamation was a significant step towards equality and justice for all individuals, and Lincoln's efforts to abolish slavery have had a lasting impact on society. Additionally, Lincoln's leadership and dedication to upholding the principles of democracy and freedom have inspired generations of Americans and individuals around the world.\""
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "composed_chain.invoke({\"politician\": \"Lincoln\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b650952f-a0b1-404e-ac63-1a11ed4f347e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Attila the Hun's conquests and actions were not necessarily positive for humanity. He was known for his brutal tactics, including pillaging and destroying cities, and his reign brought suffering and devastation to many people. While he was certainly a powerful and formidable leader, his legacy is more often associated with violence and destruction rather than positive contributions to humanity.\""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "composed_chain.invoke({\"politician\": \"Attila\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e7f6e6-7a4c-481b-b71a-445a291029ea",
   "metadata": {},
   "source": [
    "### Functions can also be included in Runnables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7516f36d-d166-4534-a487-a103dbd1edea",
   "metadata": {},
   "outputs": [],
   "source": [
    "composed_chain_with_lambda = (\n",
    "    chain\n",
    "    | (lambda input: {\"politician\": input})\n",
    "    | historian_prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a7431d79-fa8f-4587-9c78-aae02acc0907",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"It is a matter of debate whether Robespierre's actions during the Reign of Terror were ultimately positive for humanity. While he did play a key role in the French Revolution and advocated for radical social and political change, his methods were often brutal and oppressive. The Reign of Terror led to the deaths of thousands of people, many of whom were executed without fair trial. Some argue that Robespierre's actions were necessary to protect the revolution and establish a new order, while others believe that the violence and repression of the Reign of Terror ultimately did more harm than good. Ultimately, the question of whether Robespierre was a positive figure for humanity is a complex and contentious one.\""
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "composed_chain_with_lambda.invoke({\"politician\": \"Robespierre\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e4cebf2-c24f-4bb5-bb8f-bcd2daf72f82",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Multiple chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "172fd045-567e-40bc-8e42-e8f3923ca1a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Le continent où se trouve la France, dont François Mitterrand était originaire, est l'Europe.\""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "prompt1 = ChatPromptTemplate.from_template(\"what is the country {politician} is from?\")\n",
    "prompt2 = ChatPromptTemplate.from_template(\n",
    "    \"what continent is the country {country} in? respond in {language}\"\n",
    ")\n",
    "\n",
    "model = ChatOpenAI()\n",
    "\n",
    "chain1 = prompt1 | model | StrOutputParser()\n",
    "\n",
    "chain2 = (\n",
    "    {\"country\": chain1, \"language\": itemgetter(\"language\")}\n",
    "    | prompt2\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "chain2.invoke({\"politician\": \"Miterrand\", \"language\": \"French\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd47ead9-7fb7-4fa2-951e-50026e04b43c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Nested chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b788e91c-3afe-461c-9080-086c37f26a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain_core.runnables import RunnableParallel\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"tell me a curious fact about {soccer_player}\")\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "def russian_lastname_from_dictionary(person):\n",
    "    return person[\"name\"] + \"ovich\"\n",
    "\n",
    "chain = RunnableParallel(\n",
    "    {\n",
    "        \"soccer_player\": RunnablePassthrough() \n",
    "        | RunnableLambda(russian_lastname_from_dictionary), \n",
    "        \"operation_c\": RunnablePassthrough()\n",
    "    }\n",
    ") | prompt | model | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cc6d31c5-be72-4c95-80ab-944843450e32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"One curious fact about Roman Abramovich is that he is known to be a collector of rare and expensive art pieces. He has an extensive art collection that includes works by renowned artists such as Francis Bacon, Lucian Freud, and Andy Warhol. Abramovich's love for art is reflected in the design of his luxury yacht, which features a collection of expensive artworks displayed throughout the vessel.\""
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\n",
    "    \"name1\": \"Jordam\",\n",
    "    \"name\": \"Abram\"\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b7e51df-c2fc-47d5-85d4-c4717bf32603",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Fallback for chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "509b9f90-648c-48da-acbf-206519e74966",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First let's create a chain with a ChatModel\n",
    "# We add in a string output parser here so the outputs between the two are the same type\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You're a funny assistant who always includes a joke in your response\",\n",
    "        ),\n",
    "        (\"human\", \"Who is the best {sport} player worldwide?\"),\n",
    "    ]\n",
    ")\n",
    "# Here we're going to use a bad model name to easily create a chain that will error\n",
    "chat_model = ChatOpenAI(model=\"gpt-fake\")\n",
    "\n",
    "bad_chain = chat_prompt | chat_model | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d75321e-ec9c-41a3-9377-1b26d7ced7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now lets create a chain with the normal OpenAI model\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import OpenAI\n",
    "\n",
    "prompt_template = \"\"\"Instructions: You're a funny assistant who always includes a joke in your response.\n",
    "\n",
    "Question: Who is the best {sport} player worldwide?\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(prompt_template)\n",
    "\n",
    "llm = OpenAI()\n",
    "\n",
    "good_chain = prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "30b6231c-ef6e-4d18-ba22-b92888e515cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\nResponse: Well, it depends on who you ask. Some might say Messi, others might say Ronaldo. But I personally think it's my grandma, she can kick a ball farther than anyone I know!\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can now create a final chain which combines the two\n",
    "chain = bad_chain.with_fallbacks([good_chain])\n",
    "\n",
    "chain.invoke({\"sport\": \"soccer\"})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
