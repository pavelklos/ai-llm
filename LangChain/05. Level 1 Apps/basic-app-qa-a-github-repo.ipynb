{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb26d669-80c7-4132-9a79-7cf6680538e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n",
    "openai_api_key = os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1217b85-bda5-48a2-a733-3040a1ee38e0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Basic app for QA code library or github repository"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "020a520a-08b8-423b-bbf6-6bc8fb316769",
   "metadata": {},
   "source": [
    "### Load github repo 'The Fuzz' (small python module for string matching)\n",
    "- [PyPI](https://pypi.org/project/thefuzz/)\n",
    "- [GitHub](https://github.com/seatgeek/thefuzz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d08285d1-544d-4cd0-a8d8-94939364fc62",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = \"data/thefuzz-master\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5dbf3eca-fba9-402d-af47-d031cb4006d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "document_chunks = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55d4b606-2f7b-471b-a319-5e66a4898a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e8843188-e15e-48d0-8fa8-7cb7a347f727",
   "metadata": {},
   "outputs": [],
   "source": [
    "for dirpath, dirnames, filenames in os.walk(root_dir):\n",
    "    for file in filenames:\n",
    "        try:\n",
    "            loader = TextLoader(\n",
    "                os.path.join(dirpath, file),\n",
    "                encoding=\"utf-8\"\n",
    "            )\n",
    "            document_chunks.extend(loader.load_and_split())\n",
    "        except Exception as e:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ca1dc28-3ca5-4f69-a342-eb509370045c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 170 chunks.\n"
     ]
    }
   ],
   "source": [
    "print(f\"We have {len(document_chunks)} chunks.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c7e32912-94b7-41eb-99e1-7cf8d44818bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import unittest\n",
      "import re\n",
      "import pycodestyle\n",
      "\n",
      "from thefuzz import fuzz\n",
      "from thefuzz import process\n",
      "from thefuzz import utils\n",
      "\n",
      "scorers = [\n",
      "    fuzz.ratio,\n",
      "    fuzz.partial_ratio,\n",
      "    fuzz.token_sort_ratio,\n",
      "    fuzz.token_set_ratio,\n",
      "    fuzz.partial_token_sort_ratio,\n",
      "    fuzz.partial_token_set_ratio,\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(document_chunks[0].page_content[:300])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9713e800-cc92-45f2-a8c1-dc374d24461a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Convert text chunks in embeddings and store them in vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb255432-d1b0-4711-b5f9-f2ab8c7e84cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9368b26b-0506-4a37-84f5-83e145780efb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/juliocolomer/.pyenv/versions/3.11.4/envs/venv020124/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.embeddings.openai.OpenAIEmbeddings` was deprecated in langchain-community 0.1.0 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAIEmbeddings`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ba2daa7c-f7f4-47e6-938a-61739856ca6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "stored_embeddings = FAISS.from_documents(document_chunks, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe95a1e-e319-4502-b352-bddb102bb7ff",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Create RetrievalQA chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e0fdba87-5765-4d50-b57f-7e6a8fbe6d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cbdc197b-e4b3-420c-979e-22da1831c6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_model = ChatOpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "554e2f41-e258-446e-bff1-19f94b5d2991",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "54e20784-cde2-45e8-824e-09eb2b02aa41",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=chat_model,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=stored_embeddings.as_retriever()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e8c251-35da-48fe-8e10-45846a2520ff",
   "metadata": {},
   "source": [
    "### Now we can make questions about github library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "673e2df3-f652-4f08-a03b-21b23539ca17",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"\"\"\n",
    "What function do I use if I want to find \n",
    "the most similar item in a list of items?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bf32dec0-3e20-49f2-967f-2c0203684281",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/juliocolomer/.pyenv/versions/3.11.4/envs/venv020124/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `run` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "answer = qa_chain.run(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2b4a0e66-b122-4d01-bb49-24a572b28cb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You can use the `process.extractOne()` function from the `thefuzz` library to find the most similar item in a list of items. This function takes a query string and a list of choices, and it returns a tuple containing the best match and its similarity score. Here's an example of how to use it:\n",
      "\n",
      "```python\n",
      "from thefuzz import process\n",
      "\n",
      "choices = [\"apple\", \"banana\", \"orange\"]\n",
      "query = \"aple\"\n",
      "\n",
      "best_match = process.extractOne(query, choices)\n",
      "print(best_match)\n",
      "```\n",
      "\n",
      "Output:\n",
      "```\n",
      "('apple', 80)\n",
      "```\n",
      "\n",
      "In this example, the best match for the query \"aple\" is \"apple\" with a similarity score of 80.\n"
     ]
    }
   ],
   "source": [
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73bdd0d7-a245-4d58-8243-206fa97477de",
   "metadata": {},
   "source": [
    "## GitHub Repository Q&A System with DeepLake Vector Store\n",
    "Complete example that loads data from a GitHub repository into a DeepLake vector store and allows you to ask questions about it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5dfd9a-bb0d-4784-abdb-b373e231507f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install langchain langchain-openai langchain-community langchain-core python-dotenv gitpython deeplake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a7be2e-cec8-4151-9e8e-ea91176c71f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tempfile\n",
    "from git import Repo\n",
    "from dotenv import load_dotenv\n",
    "from langchain_core.documents import Document\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_community.vectorstores import DeepLake\n",
    "from langchain_community.document_loaders.generic import GenericLoader\n",
    "from langchain_community.document_loaders.parsers import LanguageParser\n",
    "from langchain.chains.retrieval_qa.base import RetrievalQA\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Check if OpenAI API key is available\n",
    "if \"OPENAI_API_KEY\" not in os.environ:\n",
    "    raise ValueError(\"Please set OPENAI_API_KEY in your environment variables or .env file\")\n",
    "\n",
    "# Function to clone a GitHub repository and load its contents\n",
    "def load_github_repo(repo_url, branch=\"main\", file_extensions=[\".py\", \".js\", \".md\", \".txt\"]):\n",
    "    \"\"\"\n",
    "    Clone a GitHub repository and load its contents as documents.\n",
    "    \n",
    "    Args:\n",
    "        repo_url: URL of the GitHub repository\n",
    "        branch: Branch to clone (default: main)\n",
    "        file_extensions: List of file extensions to load\n",
    "    \n",
    "    Returns:\n",
    "        List of Document objects\n",
    "    \"\"\"\n",
    "    print(f\"Cloning repository: {repo_url}, branch: {branch}\")\n",
    "    \n",
    "    # Create a temporary directory for the repo\n",
    "    with tempfile.TemporaryDirectory() as temp_dir:\n",
    "        # Clone the repository\n",
    "        repo = Repo.clone_from(repo_url, temp_dir, branch=branch)\n",
    "        \n",
    "        # Set up the loader for code files\n",
    "        loader = GenericLoader.from_filesystem(\n",
    "            temp_dir,\n",
    "            glob=\"**/*\",\n",
    "            suffixes=file_extensions,\n",
    "            parser=LanguageParser()\n",
    "        )\n",
    "        \n",
    "        # Load documents from the repo\n",
    "        documents = loader.load()\n",
    "        print(f\"Loaded {len(documents)} documents from repository\")\n",
    "        \n",
    "        return documents\n",
    "\n",
    "# Set up the GitHub repository to load\n",
    "repo_url = \"https://github.com/langchain-ai/langchain\"  # You can change this to any repo\n",
    "branch = \"master\"  # Change if needed\n",
    "file_extensions = [\".py\", \".md\", \".txt\"]  # Extensions to load\n",
    "\n",
    "# Load documents from the GitHub repository\n",
    "documents = load_github_repo(repo_url, branch, file_extensions)\n",
    "\n",
    "# Split the documents into smaller chunks for better retrieval\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    "    length_function=len,\n",
    ")\n",
    "split_docs = text_splitter.split_documents(documents)\n",
    "\n",
    "print(f\"Split into {len(split_docs)} chunks\")\n",
    "\n",
    "# Initialize OpenAI embeddings\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# Define the DeepLake dataset path\n",
    "dataset_path = \"deeplake_github_store\"\n",
    "\n",
    "# Create and load data into the DeepLake vector store\n",
    "vector_store = DeepLake.from_documents(\n",
    "    split_docs, \n",
    "    embeddings, \n",
    "    dataset_path=dataset_path,\n",
    "    overwrite=True  # Set to False if you want to add to existing store\n",
    ")\n",
    "\n",
    "print(\"Vector store created successfully!\")\n",
    "\n",
    "# Create a retriever from the vector store\n",
    "retriever = vector_store.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": 4}  # Return top 4 most relevant documents\n",
    ")\n",
    "\n",
    "# Initialize the language model\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "# Create a question-answering chain\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True\n",
    ")\n",
    "\n",
    "# Function to ask questions about the repository\n",
    "def ask_repo(question):\n",
    "    \"\"\"\n",
    "    Ask a question about the GitHub repository\n",
    "    \n",
    "    Args:\n",
    "        question: Question to ask\n",
    "    \n",
    "    Returns:\n",
    "        Answer from the QA chain\n",
    "    \"\"\"\n",
    "    result = qa_chain.invoke({\"query\": question})\n",
    "    \n",
    "    print(f\"\\nQuestion: {question}\")\n",
    "    print(f\"\\nAnswer: {result['result']}\")\n",
    "    print(\"\\nSources:\")\n",
    "    for i, doc in enumerate(result[\"source_documents\"]):\n",
    "        print(f\"\\n{i+1}. {doc.metadata.get('source', 'Unknown source')}\")\n",
    "        print(f\"   Content: {doc.page_content[:150]}...\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Example questions to ask about the repository\n",
    "questions = [\n",
    "    \"What is this repository about?\",\n",
    "    \"How does the LangChain framework work?\",\n",
    "    \"What are the main components of LangChain?\",\n",
    "    \"Show me an example of code from this repository\"\n",
    "]\n",
    "\n",
    "# Ask the questions\n",
    "for question in questions:\n",
    "    ask_repo(question)\n",
    "\n",
    "# Interactive mode\n",
    "def interactive_qa():\n",
    "    \"\"\"Run an interactive Q&A session\"\"\"\n",
    "    print(\"\\n\\n--- Interactive Q&A Mode ---\")\n",
    "    print(\"Type 'exit' to quit\")\n",
    "    \n",
    "    while True:\n",
    "        question = input(\"\\nQuestion: \")\n",
    "        if question.lower() in [\"exit\", \"quit\", \"q\"]:\n",
    "            break\n",
    "        \n",
    "        ask_repo(question)\n",
    "\n",
    "# Start interactive mode\n",
    "interactive_qa()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
