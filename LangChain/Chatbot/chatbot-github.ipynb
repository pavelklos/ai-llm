{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7cce71e0-bff0-46a5-8c37-aed48ad95be8",
   "metadata": {},
   "source": [
    "## LLM RAG (Retrieval Augmented Generation) application that can load and query GitHub repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a14aa355-83c0-47a8-bcde-227edd76a374",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [REQUEST]\n",
    "#   Create LLM Rag APP asking GitHub repo url?\n",
    "# [RESPONSE]\n",
    "#   I'll help you create a LLM RAG (Retrieval Augmented Generation) application\n",
    "#   that can load and query a GitHub repository. Based on the LangChain documentation,\n",
    "#   here's a comprehensive example:\n",
    "\n",
    "# [Install required packages]\n",
    "# pip install langchain langchain-community langchain-openai chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99800da9-9087-499f-b056-ea66fa8e1be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_community.document_loaders import GitLoader\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abdbd0c9-fba6-4503-b5a9-38cfa205af9c",
   "metadata": {},
   "source": [
    "### Set up OpenAI API Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf846d40-9e10-48d4-ad7d-87264070ec57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up OpenAI API Key\n",
    "# if \"OPENAI_API_KEY\" not in os.environ:\n",
    "#     os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter OpenAI API Key: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45a74568-0e95-4375-9e30-8e3e159df726",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n",
    "# openai_api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "\n",
    "# load_dotenv(override = True)\n",
    "# api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "MODEL_GPT=\"gpt-4o-mini\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2bbcf5d-5cec-4dee-b375-f5826698a9a5",
   "metadata": {},
   "source": [
    "### Setup steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "52332e0a-b5bc-42fd-9c94-6340ee05b834",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_url = \"https://github.com/pavelklos/ai-llm\"\n",
    "repo_path = \"ai-llm-repo\"  # Local path to clone repo\n",
    "\n",
    "# Load only .py files from '/LangChain/Chatbot/' folder\n",
    "# ai-llm-repo/LangChain/Chatbot/chatbot-database.py\n",
    "# ai-llm-repo/LangChain/Chatbot/chatbot-github.py\n",
    "# ai-llm-repo/LangChain/Chatbot/chatbot-rag-pdf-faiss.py\n",
    "def is_valid_file(file_path):\n",
    "    file_path_replaced = file_path.replace('\\\\', '/')\n",
    "    # print(file_path_replaced)\n",
    "    starts_with = f\"{repo_path}/LangChain/Chatbot/\"\n",
    "    ends_with = \".py\"\n",
    "    if (file_path_replaced.startswith(starts_with) and file_path_replaced.endswith(ends_with)):\n",
    "        print(file_path_replaced)\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "loader = GitLoader(\n",
    "    clone_url=repo_url,\n",
    "    repo_path=repo_path,\n",
    "    branch=\"main\",\n",
    "    file_filter=is_valid_file  # Callable[[str], bool]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63c6ec28-6fdb-46b3-b161-409d762037de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ai-llm-repo/LangChain/Chatbot/chatbot-database.py\n",
      "ai-llm-repo/LangChain/Chatbot/chatbot-github.py\n",
      "ai-llm-repo/LangChain/Chatbot/chatbot-rag-pdf-faiss.py\n"
     ]
    }
   ],
   "source": [
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e33ded64-a424-4a41-8f29-0a9d8298173e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "print(len(documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0bc60d88-f42c-463b-827a-b6081e79cbf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: LangChain\\Chatbot\\chatbot-database.py\n",
      "# [REQUEST]\n",
      "#   Create sample code to query SQLite database by LLM, LangChain and SQLDatabaseChain?\n",
      "# [RESPONSE]\n",
      "#   I'll provide a comprehensive example of querying a SQLite database using\n",
      "#   La\n",
      "\n",
      "Loaded: LangChain\\Chatbot\\chatbot-github.py\n",
      "# [REQUEST]\n",
      "#   Create LLM Rag APP asking GitHub repo url?\n",
      "# [RESPONSE]\n",
      "#   I'll help you create a LLM RAG (Retrieval Augmented Generation) application\n",
      "#   that can load and query a GitHub reposit\n",
      "\n",
      "Loaded: LangChain\\Chatbot\\chatbot-rag-pdf-faiss.py\n",
      "# [REQUEST]\n",
      "#   Create basic RAG App with PDF locally and FAISS vector store, save and load vector store?\n",
      "# [RESPONSE]\n",
      "#   I'll provide a comprehensive example of creating a RAG application with a \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for doc in documents:\n",
    "    print(f\"Loaded: {doc.metadata['source']}\")\n",
    "    # print(f\"Loaded: {doc.metadata['file_path']}\")\n",
    "    print(f\"{doc.page_content[:200]}\\n\")  # Print first 200 characters for preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f0e8a9e3-2533-4946-a678-77d6a5db3b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c2ffa07e-00d3-4e70-993d-6c202c445435",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = Chroma.from_documents(\n",
    "    documents=documents, \n",
    "    embedding=embeddings\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2e8ae951-3312-4b6a-b6e2-af56ef0f4c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": len(documents)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8fa8978d-6af5-4b03-8f61-befd1450487a",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model=MODEL_GPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "71870ab4-9d9b-4870-af89-e06e16d03f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "You are an expert code and documentation assistant. \n",
    "Answer the question based ONLY on the following context:\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "If the information is not in the context, say \"I cannot find the answer in the repository.\"\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1298ed19-b6d6-4947-93da-e0def0f81532",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae691b5-6d07-4efe-ad75-d09b6a64e3a9",
   "metadata": {},
   "source": [
    "### Example usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "927ca8a4-755b-49d5-8601-617c39527acc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question: Summarize the code in each document, shortly (200 words)\n",
      "Answer: 1. **chatbot-rag-pdf-faiss.py**: This script implements a Retrieval-Augmented Generation (RAG) application that loads a PDF document, processes it, and utilizes a FAISS vector store for efficient querying. The process includes loading a PDF, splitting its content into manageable chunks, creating a vector store from these chunks using OpenAI embeddings, and providing functionality to save and load the vector store locally. The main execution flow sets up the OpenAI API key, processes a specified PDF, saves the vector store, and then demonstrates querying the document via a RAG chain that retrieves relevant context and generates answers based on user queries.\n",
      "\n",
      "2. **chatbot-github.py**: This code outlines the creation of a RAG application that queries a GitHub repository. It involves cloning the repository, loading relevant documents (e.g., Python files, markdown), and creating embeddings. A Chroma vector store is generated from the documents, which is then used to set up a retriever and a language model (LLM). The application constructs a prompt template and creates a RAG chain for querying. The main function allows users to input questions about the repository, and the app returns corresponding answers.\n",
      "\n",
      "3. **chatbot-database.py**: This script provides a framework for querying an SQLite database using LangChain's SQLDatabaseChain. It establishes a connection to a specified SQLite database, initializes a language model, and creates a SQL database chain to handle queries. The code includes a helper function for executing database queries and error handling. An example usage scenario demonstrates how to run natural language queries against the database and print the results, showcasing the interaction with the database through user queries.\n"
     ]
    }
   ],
   "source": [
    "questions = [\n",
    "    \"Summarize the code in each document, shortly (200 words)\",\n",
    "    # \"\"\n",
    "]\n",
    "    \n",
    "for question in questions:\n",
    "    print(f\"\\nQuestion: {question}\")\n",
    "    answer = rag_chain.invoke(question)\n",
    "    print(f\"Answer: {answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6799523d-a8bb-46e8-9f29-d90ff6c4a6a6",
   "metadata": {},
   "source": [
    "## [chatbot-github.py]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e2e6e26b-bd4e-46e1-b39a-6a4bf7bb8c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Required imports\n",
    "# import os\n",
    "# from langchain_community.document_loaders import GitLoader\n",
    "# from langchain_openai import OpenAIEmbeddings\n",
    "# from langchain_community.vectorstores import Chroma\n",
    "# from langchain_openai import ChatOpenAI\n",
    "# from langchain_core.prompts import ChatPromptTemplate\n",
    "# from langchain_core.runnables import RunnablePassthrough\n",
    "# from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# # Setup steps\n",
    "# def create_github_rag_app(repo_url, branch=\"master\"):\n",
    "#     # 1. Clone the repository\n",
    "#     repo_path = \"./example_data/github_repo/\"\n",
    "    \n",
    "#     # Load repository\n",
    "#     loader = GitLoader(\n",
    "#         clone_url=repo_url,\n",
    "#         repo_path=repo_path,\n",
    "#         branch=branch,\n",
    "#         # Optional: filter only specific file types\n",
    "#         file_filter=lambda file_path: file_path.endswith((\".py\", \".md\", \".txt\"))\n",
    "#     )\n",
    "    \n",
    "#     # 2. Load documents\n",
    "#     documents = loader.load()\n",
    "    \n",
    "#     # 3. Create embeddings\n",
    "#     embeddings = OpenAIEmbeddings()\n",
    "    \n",
    "#     # 4. Create vector store\n",
    "#     vectorstore = Chroma.from_documents(\n",
    "#         documents=documents, \n",
    "#         embedding=embeddings\n",
    "#     )\n",
    "    \n",
    "#     # 5. Create retriever\n",
    "#     retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})\n",
    "    \n",
    "#     # 6. Create LLM\n",
    "#     llm = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "    \n",
    "#     # 7. Create prompt template\n",
    "#     prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "#     You are an expert code and documentation assistant. \n",
    "#     Answer the question based ONLY on the following context:\n",
    "    \n",
    "#     {context}\n",
    "    \n",
    "#     Question: {question}\n",
    "    \n",
    "#     If the information is not in the context, say \"I cannot find the answer in the repository.\"\n",
    "#     \"\"\")\n",
    "    \n",
    "#     # 8. Create RAG chain\n",
    "#     rag_chain = (\n",
    "#         {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "#         | prompt\n",
    "#         | llm\n",
    "#         | StrOutputParser()\n",
    "#     )\n",
    "    \n",
    "#     return rag_chain\n",
    "\n",
    "# # Example usage\n",
    "# def main():\n",
    "#     # Set your OpenAI API key\n",
    "#     os.environ[\"OPENAI_API_KEY\"] = \"your-openai-api-key\"\n",
    "    \n",
    "#     # Create RAG app for a specific GitHub repo\n",
    "#     repo_url = \"https://github.com/langchain-ai/langchain\"\n",
    "#     rag_app = create_github_rag_app(repo_url)\n",
    "    \n",
    "#     # Ask questions\n",
    "#     questions = [\n",
    "#         \"What is LangChain used for?\",\n",
    "#         \"Explain the main components of LangChain\",\n",
    "#         \"How do I create a basic RAG application?\"\n",
    "#     ]\n",
    "    \n",
    "#     for question in questions:\n",
    "#         print(f\"\\nQuestion: {question}\")\n",
    "#         answer = rag_app.invoke(question)\n",
    "#         print(f\"Answer: {answer}\")\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
